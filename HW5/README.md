# Replicated Experiment

## Experiment Setting
### 1.Distillation Model
In this experiment, we compare the experiments of three methods on four ETT datasets. When running the experiment on knowledge distillation, you need to comment the corresponding code in the file /Users/weimuhao/Desktop/HW5/src/models/DLbase.py.(due to the circle import, i reworte the Transformer model in this file.) Parameters are as follows: lr=0.01, epochs=100, alpha=0.1.
### 2.Data Augmentation & Add Linear Layer & Different Alpha Values
The specific details of the experiment have been explained in the experiment report and can be repeated according to the content of the experiment report, so it will not be repeated here.

## Tips
The specific experimental data are encapsulated in.zip file and can be reused directly

## Configs
numpy, torch, matplotlib, pandas, einops, reformer_pytorch, typing
